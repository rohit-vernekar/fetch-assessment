{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Sentence Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformerModel(nn.Module):\n",
    "    def __init__(self, transformer_name=\"distilbert-base-uncased\"):\n",
    "        super(SentenceTransformerModel, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(transformer_name)\n",
    "        self.transformer = AutoModel.from_pretrained(transformer_name)\n",
    "        \n",
    "    def encode(self, sentences):\n",
    "        # Tokenize and encode sentences\n",
    "        inputs = self.tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        outputs = self.transformer(**inputs)\n",
    "        \n",
    "        # Using the embedding of the [CLS] token for sentence representation\n",
    "        # Other approach could be obtaining the embeddings by averaging the last hidden states across the sequence\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] token is at index 0\n",
    "        \n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: tensor([[-0.0577,  0.0290, -0.0846,  ..., -0.0329,  0.3543,  0.3237],\n",
      "        [-0.1678, -0.3774,  0.0050,  ..., -0.2765,  0.3918,  0.0882]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Length: torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "# Testing the model with sample sentences\n",
    "model = SentenceTransformerModel()\n",
    "test_sentences = [\"I love machine learning!\", \"Transformers are powerful models.\"]\n",
    "embeddings = model.encode(test_sentences)\n",
    "print(\"Embeddings:\", embeddings)\n",
    "# Each sentence is represented by a vector of size 768\n",
    "print(\"Length:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Multi-Task Learning Expansion\n",
    "\n",
    "The dataset used contains six of the review topics used in the paper *John Blitzer, Mark Dredze, and Fernando Pereira: Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL 2007).*\n",
    "\n",
    "The data has been formatted so that there is one review per line, and the texts have been tokenized and normalized. \n",
    "\n",
    "A line in the file is organized in columns as follows:\n",
    "\n",
    "0: <b> topic</b> category label (books, camera, dvd, health, music, or software) <br>\n",
    "1: <b>sentiment</b> category label (pos or neg) <br>\n",
    "2: document identifier <br>\n",
    "3 and on: the actual review  <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Reviews: 11914\n",
      "Label: neg\n",
      "Class: music\n",
      "Review: i bought this album because i loved the title song . it 's such a great song , how bad can the rest of the album be , right ? well , the rest of the songs are just filler and are n't worth the money i paid for this . it 's either shameless bubblegum or oversentimentalized depressing tripe . kenny chesney is a popular artist and as a result he is in the cookie cutter category of the nashville music scene . he 's gotta pump out the albums so the record company can keep lining their pockets while the suckers out there keep buying this garbage to perpetuate more garbage coming out of that town . i 'll get down off my soapbox now . but country music really needs to get back to it 's roots and stop this pop nonsense . what country music really is and what it is considered to be by mainstream are two different things . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "with open(\"data/all_reviews.txt\") as fp:\n",
    "    data = fp.readlines()\n",
    "    all_classes = [review.split(\" \")[0] for review in data]\n",
    "    all_labels = [review.split(\" \")[1] for review in data]\n",
    "    all_reviews = [\" \".join(review.split(\" \")[3:]) for review in data]\n",
    "\n",
    "print(f\"Number of Reviews: {len(all_reviews)}\")\n",
    "\n",
    "# Printing 1\n",
    "print(\"Label:\", all_labels[0])\n",
    "print(\"Class:\", all_classes[0])\n",
    "print(\"Review:\", all_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only 500 reviews\n",
    "n_samples = 500\n",
    "all_reviews = all_reviews[:n_samples]\n",
    "all_labels = all_labels[:n_samples]\n",
    "all_classes = all_classes[:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Labels: {'pos', 'neg'}\n",
      "Unique Classes: {'software', 'books', 'music', 'dvd', 'camera', 'health'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique Labels:\", set(all_labels))\n",
    "print(\"Unique Classes:\", set(all_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the labels and classes\n",
    "label_encoder = {\"pos\": 1, \"neg\": 0}\n",
    "class_encoder = {\"books\": 0, \"camera\": 1, \"dvd\": 2, \"health\": 3, \"music\": 4, \"software\": 5}\n",
    "label_decoder = {v: k for k, v in label_encoder.items()}\n",
    "class_decoder = {v: k for k, v in class_encoder.items()}\n",
    "\n",
    "all_classes = torch.tensor([class_encoder[cls] for cls in all_classes])\n",
    "all_labels = torch.tensor([label_encoder[label] for label in all_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 400, Test Size: 100\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into train and test\n",
    "split_point = 0.8\n",
    "split_idx = int(len(all_reviews) * split_point)\n",
    "    \n",
    "train_reviews = all_reviews[:split_idx]\n",
    "test_reviews = all_reviews[split_idx:]\n",
    "train_labels = all_labels[:split_idx]\n",
    "test_labels = all_labels[split_idx:]\n",
    "train_classes = all_classes[:split_idx]\n",
    "test_classes = all_classes[split_idx:]\n",
    "\n",
    "print(f\"Train Size: {len(train_reviews)}, Test Size: {len(test_reviews)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskModel(SentenceTransformerModel):\n",
    "    def __init__(self, transformer_name=\"distilbert-base-uncased\"):\n",
    "        super().__init__(transformer_name)\n",
    "        \n",
    "        # Freezing the transformer backbone weights\n",
    "        for param in self.transformer.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.sentiment_classifier = nn.Sequential(\n",
    "            nn.Linear(self.transformer.config.hidden_size, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.sentence_classifier = nn.Sequential(\n",
    "            nn.Linear(self.transformer.config.hidden_size, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 6),  # Output layer for 6 classes\n",
    "            nn.Softmax(dim=1)  # Softmax for multi-class classification\n",
    "        )\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        # Obtain the embeddings from the transformer backbone\n",
    "        embeddings = self.encode(sentences)\n",
    "        sentiment_pred = self.sentiment_classifier(embeddings)\n",
    "        class_pred = self.sentence_classifier(embeddings)\n",
    "        return sentiment_pred, class_pred\n",
    "    \n",
    "    def loss(self, sentences, labels, classes):\n",
    "        sentiment_pred, class_pred = self.forward(sentences)\n",
    "\n",
    "        loss = F.binary_cross_entropy(input=sentiment_pred.squeeze(), target=labels.float())\n",
    "        loss += F.cross_entropy(input=class_pred, target=classes)\n",
    "        return loss\n",
    "\n",
    "model = MultiTaskModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer-Wise Learning Rate Implementation\n",
    "\n",
    "Layer-wise learning rates is defined to set higher rates for the top layers and lower rates for the earlier (more general) layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "transformer.layer.0.attention.q_lin.weight\n",
      "transformer.layer.0.attention.q_lin.bias\n",
      "transformer.layer.0.attention.k_lin.weight\n",
      "transformer.layer.0.attention.k_lin.bias\n",
      "transformer.layer.0.attention.v_lin.weight\n",
      "transformer.layer.0.attention.v_lin.bias\n",
      "transformer.layer.0.attention.out_lin.weight\n",
      "transformer.layer.0.attention.out_lin.bias\n",
      "transformer.layer.0.sa_layer_norm.weight\n",
      "transformer.layer.0.sa_layer_norm.bias\n",
      "transformer.layer.0.ffn.lin1.weight\n",
      "transformer.layer.0.ffn.lin1.bias\n",
      "transformer.layer.0.ffn.lin2.weight\n",
      "transformer.layer.0.ffn.lin2.bias\n",
      "transformer.layer.0.output_layer_norm.weight\n",
      "transformer.layer.0.output_layer_norm.bias\n",
      "transformer.layer.1.attention.q_lin.weight\n",
      "transformer.layer.1.attention.q_lin.bias\n",
      "transformer.layer.1.attention.k_lin.weight\n",
      "transformer.layer.1.attention.k_lin.bias\n",
      "transformer.layer.1.attention.v_lin.weight\n",
      "transformer.layer.1.attention.v_lin.bias\n",
      "transformer.layer.1.attention.out_lin.weight\n",
      "transformer.layer.1.attention.out_lin.bias\n",
      "transformer.layer.1.sa_layer_norm.weight\n",
      "transformer.layer.1.sa_layer_norm.bias\n",
      "transformer.layer.1.ffn.lin1.weight\n",
      "transformer.layer.1.ffn.lin1.bias\n",
      "transformer.layer.1.ffn.lin2.weight\n",
      "transformer.layer.1.ffn.lin2.bias\n",
      "transformer.layer.1.output_layer_norm.weight\n",
      "transformer.layer.1.output_layer_norm.bias\n",
      "transformer.layer.2.attention.q_lin.weight\n",
      "transformer.layer.2.attention.q_lin.bias\n",
      "transformer.layer.2.attention.k_lin.weight\n",
      "transformer.layer.2.attention.k_lin.bias\n",
      "transformer.layer.2.attention.v_lin.weight\n",
      "transformer.layer.2.attention.v_lin.bias\n",
      "transformer.layer.2.attention.out_lin.weight\n",
      "transformer.layer.2.attention.out_lin.bias\n",
      "transformer.layer.2.sa_layer_norm.weight\n",
      "transformer.layer.2.sa_layer_norm.bias\n",
      "transformer.layer.2.ffn.lin1.weight\n",
      "transformer.layer.2.ffn.lin1.bias\n",
      "transformer.layer.2.ffn.lin2.weight\n",
      "transformer.layer.2.ffn.lin2.bias\n",
      "transformer.layer.2.output_layer_norm.weight\n",
      "transformer.layer.2.output_layer_norm.bias\n",
      "transformer.layer.3.attention.q_lin.weight\n",
      "transformer.layer.3.attention.q_lin.bias\n",
      "transformer.layer.3.attention.k_lin.weight\n",
      "transformer.layer.3.attention.k_lin.bias\n",
      "transformer.layer.3.attention.v_lin.weight\n",
      "transformer.layer.3.attention.v_lin.bias\n",
      "transformer.layer.3.attention.out_lin.weight\n",
      "transformer.layer.3.attention.out_lin.bias\n",
      "transformer.layer.3.sa_layer_norm.weight\n",
      "transformer.layer.3.sa_layer_norm.bias\n",
      "transformer.layer.3.ffn.lin1.weight\n",
      "transformer.layer.3.ffn.lin1.bias\n",
      "transformer.layer.3.ffn.lin2.weight\n",
      "transformer.layer.3.ffn.lin2.bias\n",
      "transformer.layer.3.output_layer_norm.weight\n",
      "transformer.layer.3.output_layer_norm.bias\n",
      "transformer.layer.4.attention.q_lin.weight\n",
      "transformer.layer.4.attention.q_lin.bias\n",
      "transformer.layer.4.attention.k_lin.weight\n",
      "transformer.layer.4.attention.k_lin.bias\n",
      "transformer.layer.4.attention.v_lin.weight\n",
      "transformer.layer.4.attention.v_lin.bias\n",
      "transformer.layer.4.attention.out_lin.weight\n",
      "transformer.layer.4.attention.out_lin.bias\n",
      "transformer.layer.4.sa_layer_norm.weight\n",
      "transformer.layer.4.sa_layer_norm.bias\n",
      "transformer.layer.4.ffn.lin1.weight\n",
      "transformer.layer.4.ffn.lin1.bias\n",
      "transformer.layer.4.ffn.lin2.weight\n",
      "transformer.layer.4.ffn.lin2.bias\n",
      "transformer.layer.4.output_layer_norm.weight\n",
      "transformer.layer.4.output_layer_norm.bias\n",
      "transformer.layer.5.attention.q_lin.weight\n",
      "transformer.layer.5.attention.q_lin.bias\n",
      "transformer.layer.5.attention.k_lin.weight\n",
      "transformer.layer.5.attention.k_lin.bias\n",
      "transformer.layer.5.attention.v_lin.weight\n",
      "transformer.layer.5.attention.v_lin.bias\n",
      "transformer.layer.5.attention.out_lin.weight\n",
      "transformer.layer.5.attention.out_lin.bias\n",
      "transformer.layer.5.sa_layer_norm.weight\n",
      "transformer.layer.5.sa_layer_norm.bias\n",
      "transformer.layer.5.ffn.lin1.weight\n",
      "transformer.layer.5.ffn.lin1.bias\n",
      "transformer.layer.5.ffn.lin2.weight\n",
      "transformer.layer.5.ffn.lin2.bias\n",
      "transformer.layer.5.output_layer_norm.weight\n",
      "transformer.layer.5.output_layer_norm.bias\n"
     ]
    }
   ],
   "source": [
    "for name, p in model.transformer.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreezing some layers of the transformer and setting different learning rates within the backbone\n",
    "for name, param in model.transformer.named_parameters():\n",
    "    if \"layer.5.output_layer_norm\" in name or \"layer.5.ffn\" in name: \n",
    "        param.requires_grad = True\n",
    "\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': model.sentiment_classifier.parameters(), 'lr': 1e-3},\n",
    "    {'params': model.sentence_classifier.parameters(), 'lr': 1e-3},\n",
    "    {'params': [param for _, param in model.transformer.named_parameters() if param.requires_grad], 'lr': 5e-5}\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.1225\n",
      "Epoch 2/20, Loss: 0.1104\n",
      "Epoch 3/20, Loss: 0.0902\n",
      "Epoch 4/20, Loss: 0.078\n",
      "Epoch 5/20, Loss: 0.0726\n",
      "Epoch 6/20, Loss: 0.0686\n",
      "Epoch 7/20, Loss: 0.0678\n",
      "Epoch 8/20, Loss: 0.0662\n",
      "Epoch 9/20, Loss: 0.0656\n",
      "Epoch 10/20, Loss: 0.0631\n",
      "Epoch 11/20, Loss: 0.0597\n",
      "Epoch 12/20, Loss: 0.0607\n",
      "Epoch 13/20, Loss: 0.0594\n",
      "Epoch 14/20, Loss: 0.0621\n",
      "Epoch 15/20, Loss: 0.0599\n",
      "Early stopping: No improvement for 2 epochs\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "epochs = 20\n",
    "patience = 2  # Number of epochs to wait for improvement\n",
    "best_loss = float('inf')\n",
    "no_improvement = 0\n",
    "\n",
    "# Training \n",
    "model.train()\n",
    "train_n = len(train_reviews)\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    curr_idx = 0\n",
    "    while curr_idx < train_n:\n",
    "        # Get batch\n",
    "        reviews = train_reviews[curr_idx: min(curr_idx + batch_size, train_n)]\n",
    "        labels = train_labels[curr_idx: min(curr_idx + batch_size, train_n)]\n",
    "        classes = train_classes[curr_idx: min(curr_idx + batch_size, train_n)]\n",
    "\n",
    "        loss = model.loss(sentences=reviews, classes=classes, labels=labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        curr_idx += batch_size\n",
    "\n",
    "    avg_loss = round(total_loss / train_n, 4)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        no_improvement = 0\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "        if no_improvement >= patience:\n",
    "            print(\"Early stopping: No improvement for {} epochs\".format(patience))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Accuracy: 0.74, Class Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_n = len(test_reviews)\n",
    "correct_sentiment = correct_class = 0\n",
    "batch_size = 32\n",
    "\n",
    "with torch.no_grad():\n",
    "    curr_idx = 0\n",
    "    while curr_idx < test_n:\n",
    "        reviews = test_reviews[curr_idx: min(curr_idx + batch_size, test_n)]\n",
    "        labels = test_labels[curr_idx: min(curr_idx + batch_size, test_n)]\n",
    "        classes = test_classes[curr_idx: min(curr_idx + batch_size, test_n)]\n",
    "\n",
    "        sentiment_pred, class_pred = model(reviews)\n",
    "        sentiment_pred = (sentiment_pred.squeeze() >= 0.5).float()\n",
    "        class_pred = torch.argmax(class_pred, dim=1)\n",
    "\n",
    "        correct_sentiment += (sentiment_pred == labels).sum().item()\n",
    "        correct_class += (class_pred == classes).sum().item()\n",
    "        curr_idx += batch_size\n",
    "\n",
    "sentiment_accuracy = correct_sentiment / test_n\n",
    "class_accuracy = correct_class / test_n\n",
    "print(f\"Sentiment Accuracy: {sentiment_accuracy}, Class Accuracy: {class_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model/model_500_reviews.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To load saved model and run prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiTaskModel()\n",
    "model.load_state_dict(torch.load(\"model/model_500_reviews.pt\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: neg\n",
      "Class: music\n"
     ]
    }
   ],
   "source": [
    "sentiment_pred, class_pred = model(\"i bought this album because i loved the title song . it 's such a great song , how bad can the rest of the album be , right ? well , the rest of the songs are just filler and are n't worth the money i paid for this . it 's either shameless bubblegum or oversentimentalized depressing tripe . kenny chesney is a popular artist and as a result he is in the cookie cutter category of the nashville music scene . he 's gotta pump out the albums so the record company can keep lining their pockets while the suckers out there keep buying this garbage to perpetuate more garbage coming out of that town . i 'll get down off my soapbox now . but country music really needs to get back to it 's roots and stop this pop nonsense . what country music really is and what it is considered to be by mainstream are two different things\")\n",
    "sentiment_pred = (sentiment_pred >= 0.5).int().item()\n",
    "class_pred = torch.argmax(class_pred, dim=1).item()\n",
    "\n",
    "print(\"Label:\", label_decoder[sentiment_pred])\n",
    "print(\"Class:\", class_decoder[class_pred])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
